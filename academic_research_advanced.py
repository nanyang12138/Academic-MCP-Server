"""
Academic Research Advanced MCP Server
ä¸“æ³¨äºæ·±åº¦åˆ†æã€æ™ºèƒ½é˜…è¯»å’Œå®Œæ•´ç ”ç©¶å·¥ä½œæµ
æ”¯æŒæœ¬åœ°PDFå’Œåœ¨çº¿URLåŒé‡è¾“å…¥
"""

from typing import Any, List, Dict, Optional
import asyncio
import logging
from mcp.server.fastmcp import FastMCP
import requests
from bs4 import BeautifulSoup
import re
from collections import defaultdict
import os
from pathlib import Path
import io

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Initialize FastMCP server
mcp = FastMCP("academic-research")


# ============================================================================
# æœ¬åœ°æ–‡ä»¶æ”¯æŒå‡½æ•°
# ============================================================================

def _is_local_path(path: str) -> bool:
    """åˆ¤æ–­æ˜¯æœ¬åœ°è·¯å¾„è¿˜æ˜¯URL"""
    return os.path.exists(path) or (not path.startswith('http'))


async def _read_pdf_content(path_or_url: str) -> io.BytesIO:
    """
    è¯»å–PDFå†…å®¹ï¼ˆæ”¯æŒæœ¬åœ°æ–‡ä»¶å’ŒURLï¼‰
    
    Args:
        path_or_url: æœ¬åœ°è·¯å¾„æˆ–URL
    
    Returns:
        BytesIOå¯¹è±¡
    """
    if _is_local_path(path_or_url):
        # æœ¬åœ°æ–‡ä»¶
        logging.info(f"Reading local PDF: {path_or_url}")
        with open(path_or_url, 'rb') as f:
            return io.BytesIO(f.read())
    else:
        # URL
        logging.info(f"Downloading PDF from: {path_or_url}")
        response = await asyncio.to_thread(requests.get, path_or_url)
        return io.BytesIO(response.content)


# ============================================================================
# 1. æ·±åº¦åˆ†æå·¥å…·
# ============================================================================

@mcp.tool()
async def analyze_citation_network(
    paper_id: str,
    source: str = "semantic_scholar",
    max_depth: int = 2
) -> Dict[str, Any]:
    """
    åˆ†æè®ºæ–‡çš„å¼•ç”¨ç½‘ç»œ
    
    Args:
        paper_id: è®ºæ–‡æ ‡è¯†ç¬¦ï¼ˆDOIã€PMIDç­‰ï¼‰
        source: æ•°æ®æºï¼ˆsemantic_scholarã€pubmedç­‰ï¼‰
        max_depth: å¼•ç”¨ç½‘ç»œæ·±åº¦ï¼ˆ1-3å±‚ï¼‰
    
    Returns:
        å¼•ç”¨ç½‘ç»œå›¾è°±ï¼ŒåŒ…æ‹¬ï¼š
        - citations: å¼•ç”¨æ­¤è®ºæ–‡çš„æ–‡ç« åˆ—è¡¨
        - references: æ­¤è®ºæ–‡å¼•ç”¨çš„æ–‡ç« åˆ—è¡¨
        - citation_count: è¢«å¼•æ¬¡æ•°
        - influential_citations: æœ‰å½±å“åŠ›çš„å¼•ç”¨
        - citation_velocity: å¼•ç”¨é€Ÿåº¦ï¼ˆè¿‘æœŸè¶‹åŠ¿ï¼‰
    """
    logging.info(f"Analyzing citation network for {paper_id}")
    
    try:
        # ä½¿ç”¨Semantic Scholar API
        if source == "semantic_scholar":
            base_url = "https://api.semanticscholar.org/graph/v1"
            
            # è·å–è®ºæ–‡åŸºæœ¬ä¿¡æ¯å’Œå¼•ç”¨
            fields = "title,citations,references,citationCount,influentialCitationCount,year,authors"
            url = f"{base_url}/paper/{paper_id}?fields={fields}"
            
            response = await asyncio.to_thread(requests.get, url)
            
            if response.status_code == 200:
                data = response.json()
                
                result = {
                    "paper_id": paper_id,
                    "title": data.get("title", ""),
                    "year": data.get("year", ""),
                    "citation_count": data.get("citationCount", 0),
                    "influential_citation_count": data.get("influentialCitationCount", 0),
                    "citations": [],
                    "references": [],
                    "citation_network_stats": {
                        "total_citations": data.get("citationCount", 0),
                        "influential_citations": data.get("influentialCitationCount", 0),
                        "influence_rate": 0
                    }
                }
                
                # è®¡ç®—å½±å“ç‡
                if result["citation_count"] > 0:
                    result["citation_network_stats"]["influence_rate"] = round(
                        result["influential_citation_count"] / result["citation_count"] * 100, 2
                    )
                
                # å¤„ç†å¼•ç”¨åˆ—è¡¨
                if "citations" in data and data["citations"]:
                    for citation in data["citations"][:20]:  # é™åˆ¶20æ¡
                        result["citations"].append({
                            "paperId": citation.get("paperId", ""),
                            "title": citation.get("title", ""),
                            "year": citation.get("year", "")
                        })
                
                # å¤„ç†å‚è€ƒæ–‡çŒ®åˆ—è¡¨
                if "references" in data and data["references"]:
                    for ref in data["references"][:20]:
                        result["references"].append({
                            "paperId": ref.get("paperId", ""),
                            "title": ref.get("title", ""),
                            "year": ref.get("year", "")
                        })
                
                return result
            else:
                return {"error": f"API error: {response.status_code}"}
        
        return {"error": f"Unsupported source: {source}"}
        
    except Exception as e:
        return {"error": f"Error analyzing citation network: {str(e)}"}


@mcp.tool()
async def evaluate_paper_impact(
    paper_id: str,
    source: str = "semantic_scholar"
) -> Dict[str, Any]:
    """
    è¯„ä¼°è®ºæ–‡çš„å­¦æœ¯å½±å“åŠ›
    
    Args:
        paper_id: è®ºæ–‡æ ‡è¯†ç¬¦
        source: æ•°æ®æº
    
    Returns:
        å½±å“åŠ›è¯„ä¼°æŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
        - citation_metrics: å¼•ç”¨æŒ‡æ ‡
        - h_index_contribution: å¯¹ä½œè€…h-indexçš„è´¡çŒ®
        - field_impact: é¢†åŸŸå½±å“åŠ›
        - temporal_impact: æ—¶é—´ç»´åº¦å½±å“
        - recommendation_score: æ¨èåˆ†æ•°ï¼ˆ0-100ï¼‰
    """
    logging.info(f"Evaluating impact for {paper_id}")
    
    try:
        base_url = "https://api.semanticscholar.org/graph/v1"
        fields = "title,year,citationCount,influentialCitationCount,citations,references,fieldsOfStudy,publicationDate"
        url = f"{base_url}/paper/{paper_id}?fields={fields}"
        
        response = await asyncio.to_thread(requests.get, url)
        
        if response.status_code == 200:
            data = response.json()
            
            # è®¡ç®—å„é¡¹æŒ‡æ ‡
            citation_count = data.get("citationCount", 0)
            influential_count = data.get("influentialCitationCount", 0)
            year = data.get("year", 2024)
            current_year = 2025
            years_since_pub = max(1, current_year - year)
            
            # å¼•ç”¨é€Ÿåº¦ï¼ˆæ¯å¹´å¹³å‡å¼•ç”¨æ•°ï¼‰
            citation_velocity = citation_count / years_since_pub
            
            # å½±å“åŠ›è¯„åˆ†ï¼ˆ0-100ï¼‰
            impact_score = min(100, (
                citation_count * 0.3 +
                influential_count * 2 +
                citation_velocity * 5
            ))
            
            result = {
                "paper_id": paper_id,
                "title": data.get("title", ""),
                "citation_metrics": {
                    "total_citations": citation_count,
                    "influential_citations": influential_count,
                    "citation_velocity": round(citation_velocity, 2),
                    "years_since_publication": years_since_pub
                },
                "field_impact": {
                    "fields": data.get("fieldsOfStudy", []),
                    "cross_disciplinary": len(data.get("fieldsOfStudy", [])) > 2
                },
                "recommendation_score": round(impact_score, 2),
                "impact_level": "High" if impact_score > 70 else "Medium" if impact_score > 30 else "Emerging",
                "key_insights": []
            }
            
            # ç”Ÿæˆå…³é”®è§è§£
            if citation_velocity > 10:
                result["key_insights"].append("é«˜å¼•ç”¨é€Ÿåº¦ï¼Œè¡¨æ˜ç ”ç©¶çƒ­åº¦æŒç»­")
            if influential_count / max(citation_count, 1) > 0.3:
                result["key_insights"].append("é«˜æ¯”ä¾‹æœ‰å½±å“åŠ›å¼•ç”¨ï¼Œè¡¨æ˜ç ”ç©¶è´¨é‡ä¼˜ç§€")
            if years_since_pub < 2 and citation_count > 50:
                result["key_insights"].append("æ–°è¿‘å‘è¡¨ä½†å·²è·é«˜å¼•ç”¨ï¼Œå±äºå¿«é€Ÿå´›èµ·çš„é‡è¦ç ”ç©¶")
            
            return result
        else:
            return {"error": f"API error: {response.status_code}"}
            
    except Exception as e:
        return {"error": f"Error evaluating impact: {str(e)}"}


@mcp.tool()
async def recommend_related_papers(
    paper_id: str,
    source: str = "semantic_scholar",
    num_recommendations: int = 10,
    strategy: str = "comprehensive"
) -> List[Dict[str, Any]]:
    """
    åŸºäºå¤šç§ç­–ç•¥æ¨èç›¸å…³æ–‡çŒ®
    
    Args:
        paper_id: æºè®ºæ–‡æ ‡è¯†ç¬¦
        source: æ•°æ®æº
        num_recommendations: æ¨èæ•°é‡
        strategy: æ¨èç­–ç•¥
            - "comprehensive": ç»¼åˆæ¨èï¼ˆå¼•ç”¨+è¢«å¼•+ç›¸ä¼¼ï¼‰
            - "citations": åŸºäºå¼•ç”¨å…³ç³»
            - "similar": åŸºäºå†…å®¹ç›¸ä¼¼åº¦
            - "influential": ä¼˜å…ˆæ¨èé«˜å½±å“åŠ›è®ºæ–‡
    
    Returns:
        æ¨èè®ºæ–‡åˆ—è¡¨ï¼Œæ¯ç¯‡åŒ…å«æ¨èç†ç”±å’Œç›¸å…³æ€§åˆ†æ•°
    """
    logging.info(f"Recommending papers for {paper_id}, strategy: {strategy}")
    
    try:
        base_url = "https://api.semanticscholar.org/graph/v1"
        
        # è·å–æºè®ºæ–‡ä¿¡æ¯
        fields = "title,citations,references,fieldsOfStudy"
        url = f"{base_url}/paper/{paper_id}?fields={fields}"
        response = await asyncio.to_thread(requests.get, url)
        
        if response.status_code != 200:
            return [{"error": f"Failed to fetch source paper: {response.status_code}"}]
        
        source_paper = response.json()
        recommendations = []
        seen_ids = set()
        
        # ç­–ç•¥1: åŸºäºå¼•ç”¨å…³ç³»
        if strategy in ["comprehensive", "citations"]:
            # è¢«å¼•ç”¨æ­¤è®ºæ–‡çš„é‡è¦æ–‡ç« 
            if "citations" in source_paper:
                for citation in source_paper["citations"][:5]:
                    paper_id_val = citation.get("paperId")
                    if paper_id_val and paper_id_val not in seen_ids:
                        recommendations.append({
                            "paperId": paper_id_val,
                            "title": citation.get("title", ""),
                            "year": citation.get("year", ""),
                            "relevance_score": 0.85,
                            "recommendation_reason": "å¼•ç”¨äº†æ‚¨å…³æ³¨çš„è®ºæ–‡"
                        })
                        seen_ids.add(paper_id_val)
            
            # æ­¤è®ºæ–‡å¼•ç”¨çš„é‡è¦æ–‡ç« 
            if "references" in source_paper:
                for ref in source_paper["references"][:5]:
                    paper_id_val = ref.get("paperId")
                    if paper_id_val and paper_id_val not in seen_ids:
                        recommendations.append({
                            "paperId": paper_id_val,
                            "title": ref.get("title", ""),
                            "year": ref.get("year", ""),
                            "relevance_score": 0.80,
                            "recommendation_reason": "è¢«æ‚¨å…³æ³¨çš„è®ºæ–‡å¼•ç”¨"
                        })
                        seen_ids.add(paper_id_val)
        
        # ç­–ç•¥2: åŸºäºSemantic Scholarçš„æ¨èAPI
        try:
            rec_url = f"{base_url}/paper/{paper_id}/recommendations?fields=title,year,citationCount&limit={num_recommendations}"
            rec_response = await asyncio.to_thread(requests.get, rec_url)
            
            if rec_response.status_code == 200:
                rec_data = rec_response.json()
                if "recommendedPapers" in rec_data:
                    for rec_paper in rec_data["recommendedPapers"]:
                        paper_id_val = rec_paper.get("paperId")
                        if paper_id_val and paper_id_val not in seen_ids:
                            recommendations.append({
                                "paperId": paper_id_val,
                                "title": rec_paper.get("title", ""),
                                "year": rec_paper.get("year", ""),
                                "citationCount": rec_paper.get("citationCount", 0),
                                "relevance_score": 0.90,
                                "recommendation_reason": "AIç®—æ³•æ¨èçš„ç›¸å…³ç ”ç©¶"
                            })
                            seen_ids.add(paper_id_val)
        except Exception as e:
            logging.warning(f"Recommendation API failed: {e}")
        
        # æ ¹æ®ç­–ç•¥æ’åº
        if strategy == "influential":
            recommendations.sort(key=lambda x: x.get("citationCount", 0), reverse=True)
        else:
            recommendations.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
        
        return recommendations[:num_recommendations]
        
    except Exception as e:
        return [{"error": f"Error generating recommendations: {str(e)}"}]


# ============================================================================
# 2. æ™ºèƒ½é˜…è¯»å·¥å…·
# ============================================================================

@mcp.tool()
async def extract_pdf_fulltext(
    pdf_url: str,
    extract_sections: bool = True
) -> Dict[str, Any]:
    """
    ä»PDFä¸­æå–å…¨æ–‡å†…å®¹
    
    Args:
        pdf_url: PDFæ–‡ä»¶URL
        extract_sections: æ˜¯å¦è¯†åˆ«å¹¶æå–å„ä¸ªç« èŠ‚
    
    Returns:
        æå–çš„æ–‡æœ¬å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
        - full_text: å®Œæ•´æ–‡æœ¬
        - sections: å„ç« èŠ‚å†…å®¹ï¼ˆå¦‚æœextract_sections=Trueï¼‰
        - metadata: PDFå…ƒæ•°æ®
        - word_count: å­—æ•°ç»Ÿè®¡
    """
    logging.info(f"Extracting fulltext from PDF: {pdf_url}")
    
    try:
        # æ³¨æ„ï¼šå®é™…ä½¿ç”¨æ—¶éœ€è¦å®‰è£… PyPDF2 æˆ– pdfplumber
        # è¿™é‡Œæä¾›ä¸€ä¸ªç®€åŒ–çš„å®ç°æ¡†æ¶
        
        result = {
            "pdf_url": pdf_url,
            "extraction_status": "partial",
            "message": "PDF extraction requires PyPDF2 or pdfplumber library",
            "suggestion": "Install with: pip install PyPDF2 pdfplumber",
            "sections": {
                "abstract": "",
                "introduction": "",
                "methods": "",
                "results": "",
                "discussion": "",
                "conclusion": "",
                "references": ""
            },
            "full_text": "",
            "word_count": 0,
            "metadata": {}
        }
        
        # TODO: å®é™…çš„PDFæå–é€»è¾‘
        # import PyPDF2
        # response = requests.get(pdf_url)
        # pdf_file = io.BytesIO(response.content)
        # pdf_reader = PyPDF2.PdfReader(pdf_file)
        # ...
        
        return result
        
    except Exception as e:
        return {"error": f"Error extracting PDF: {str(e)}"}


@mcp.tool()
async def generate_paper_summary(
    paper_id: str,
    source: str = "semantic_scholar",
    summary_type: str = "comprehensive"
) -> Dict[str, str]:
    """
    è‡ªåŠ¨ç”Ÿæˆè®ºæ–‡æ‘˜è¦
    
    Args:
        paper_id: è®ºæ–‡æ ‡è¯†ç¬¦
        source: æ•°æ®æº
        summary_type: æ‘˜è¦ç±»å‹
            - "brief": ç®€çŸ­æ‘˜è¦ï¼ˆ100-200å­—ï¼‰
            - "comprehensive": ç»¼åˆæ‘˜è¦ï¼ˆ500-800å­—ï¼‰
            - "technical": æŠ€æœ¯ç»†èŠ‚æ‘˜è¦
            - "layman": é€šä¿—æ˜“æ‡‚ç‰ˆæœ¬
    
    Returns:
        ç”Ÿæˆçš„æ‘˜è¦å’Œå…³é”®ä¿¡æ¯
    """
    logging.info(f"Generating summary for {paper_id}, type: {summary_type}")
    
    try:
        base_url = "https://api.semanticscholar.org/graph/v1"
        fields = "title,abstract,year,authors,citationCount,fieldsOfStudy,tldr"
        url = f"{base_url}/paper/{paper_id}?fields={fields}"
        
        response = await asyncio.to_thread(requests.get, url)
        
        if response.status_code == 200:
            data = response.json()
            
            # æå–å…³é”®ä¿¡æ¯
            title = data.get("title", "")
            abstract = data.get("abstract", "")
            tldr_obj = data.get("tldr", {})
            tldr_text = tldr_obj.get("text", "") if tldr_obj else ""
            authors = ", ".join([a.get("name", "") for a in data.get("authors", [])[:3]])
            if len(data.get("authors", [])) > 3:
                authors += " ç­‰"
            year = data.get("year", "")
            fields = ", ".join(data.get("fieldsOfStudy", [])[:3])
            citations = data.get("citationCount", 0)
            
            # ç”Ÿæˆä¸åŒç±»å‹çš„æ‘˜è¦
            summaries = {
                "brief": f"ã€Š{title}ã€‹({year})ç”±{authors}å‘è¡¨ï¼Œç ”ç©¶é¢†åŸŸï¼š{fields}ã€‚å·²è·{citations}æ¬¡å¼•ç”¨ã€‚{tldr_text if tldr_text else (abstract[:100] + '...' if abstract else 'æš‚æ— æ‘˜è¦')}",
                
                "comprehensive": f"""
ğŸ“„ è®ºæ–‡æ ‡é¢˜ï¼š{title}
ğŸ‘¥ ä½œè€…ï¼š{authors}
ğŸ“… å‘è¡¨å¹´ä»½ï¼š{year}
ğŸ”¬ ç ”ç©¶é¢†åŸŸï¼š{fields}
ğŸ“Š å¼•ç”¨æ¬¡æ•°ï¼š{citations}

ğŸ“ æ‘˜è¦ï¼š
{abstract if abstract else 'æš‚æ— æ‘˜è¦'}

ğŸ’¡ æ ¸å¿ƒè¦ç‚¹ï¼š
{tldr_text if tldr_text else 'æš‚æ— TLDRæ‘˜è¦'}

ğŸ“ˆ å½±å“åŠ›è¯„ä¼°ï¼š
- è¢«å¼•ç”¨ {citations} æ¬¡
- ç ”ç©¶é¢†åŸŸï¼š{fields}
- å‘è¡¨è‡³ä»Šï¼š{2025-int(year) if year else 0} å¹´
                """.strip(),
                
                "technical": f"""
ğŸ”§ æŠ€æœ¯è¦ç‚¹æ€»ç»“

ç ”ç©¶ä¸»é¢˜ï¼š{title}

æ ¸å¿ƒè´¡çŒ®ï¼š
{abstract if abstract else 'æš‚æ— æ‘˜è¦'}

æŠ€æœ¯æ–¹æ³•ï¼š
{tldr_text if tldr_text else 'éœ€è¦å®Œæ•´æ–‡æœ¬åˆ†æ'}

ç ”ç©¶é¢†åŸŸï¼š{fields}
å‘è¡¨æ—¶é—´ï¼š{year}
å¼•ç”¨æƒ…å†µï¼š{citations}æ¬¡å¼•ç”¨
                """.strip(),
                
                "layman": f"""
ğŸ“– é€šä¿—è§£è¯»

è¿™ç¯‡è®ºæ–‡ç ”ç©¶ä»€ä¹ˆï¼š
{title}

ç®€å•æ¥è¯´ï¼š
{tldr_text if tldr_text else (abstract[:200] + '...' if abstract else 'æš‚æ— æ‘˜è¦')}

ä¸ºä»€ä¹ˆé‡è¦ï¼š
è¿™é¡¹ç ”ç©¶å·²ç»è¢«å…¶ä»–å­¦è€…å¼•ç”¨äº† {citations} æ¬¡ï¼Œè¯´æ˜åœ¨å­¦æœ¯ç•Œå—åˆ°å…³æ³¨ã€‚

ç ”ç©¶é¢†åŸŸï¼š{fields}
å‘è¡¨æ—¶é—´ï¼š{year}å¹´
ä½œè€…ï¼š{authors}
                """.strip()
            }
            
            return {
                "paper_id": paper_id,
                "title": title,
                "summary": summaries.get(summary_type, summaries["comprehensive"]),
                "key_info": {
                    "authors": authors,
                    "year": str(year),
                    "fields": fields,
                    "citations": citations,
                    "tldr": tldr_text
                }
            }
        else:
            return {"error": f"API error: {response.status_code}"}
            
    except Exception as e:
        return {"error": f"Error generating summary: {str(e)}"}


@mcp.tool()
async def extract_key_information(
    paper_id: str,
    source: str = "semantic_scholar",
    info_types: List[str] = None
) -> Dict[str, Any]:
    """
    ä»è®ºæ–‡ä¸­æŠ½å–å…³é”®ä¿¡æ¯
    
    Args:
        paper_id: è®ºæ–‡æ ‡è¯†ç¬¦
        source: æ•°æ®æº
        info_types: è¦æŠ½å–çš„ä¿¡æ¯ç±»å‹åˆ—è¡¨
            - "methodology": ç ”ç©¶æ–¹æ³•
            - "findings": ä¸»è¦å‘ç°
            - "limitations": ç ”ç©¶å±€é™
            - "datasets": ä½¿ç”¨çš„æ•°æ®é›†
            - "metrics": è¯„ä¼°æŒ‡æ ‡
            - "contributions": ä¸»è¦è´¡çŒ®
    
    Returns:
        æŠ½å–çš„å„ç±»å…³é”®ä¿¡æ¯
    """
    if info_types is None:
        info_types = ["methodology", "findings", "limitations"]
    
    logging.info(f"Extracting key information from {paper_id}")
    
    try:
        base_url = "https://api.semanticscholar.org/graph/v1"
        fields = "title,abstract,tldr"
        url = f"{base_url}/paper/{paper_id}?fields={fields}"
        
        response = await asyncio.to_thread(requests.get, url)
        
        if response.status_code == 200:
            data = response.json()
            abstract = data.get("abstract", "")
            tldr = data.get("tldr", {})
            
            result = {
                "paper_id": paper_id,
                "title": data.get("title", ""),
                "extracted_info": {}
            }
            
            # TLDRæ‘˜è¦
            if tldr:
                result["tldr"] = tldr.get("text", "")
            
            # åŸºäºæ‘˜è¦çš„ç®€å•ä¿¡æ¯æŠ½å–
            for info_type in info_types:
                if info_type == "methodology":
                    method_keywords = ["method", "approach", "algorithm", "technique", "framework", "model"]
                    methods = []
                    for keyword in method_keywords:
                        if keyword in abstract.lower():
                            sentences = abstract.split('.')
                            for sent in sentences:
                                if keyword in sent.lower():
                                    methods.append(sent.strip())
                    result["extracted_info"]["methodology"] = methods if methods else ["éœ€è¦å®Œæ•´æ–‡æœ¬åˆ†æ"]
                
                elif info_type == "findings":
                    finding_keywords = ["find", "found", "result", "show", "demonstrate", "reveal", "discover"]
                    findings = []
                    for keyword in finding_keywords:
                        if keyword in abstract.lower():
                            sentences = abstract.split('.')
                            for sent in sentences:
                                if keyword in sent.lower():
                                    findings.append(sent.strip())
                    result["extracted_info"]["findings"] = findings if findings else ["éœ€è¦å®Œæ•´æ–‡æœ¬åˆ†æ"]
                
                elif info_type == "limitations":
                    limit_keywords = ["limitation", "challenge", "weakness", "drawback", "constraint"]
                    limitations = []
                    for keyword in limit_keywords:
                        if keyword in abstract.lower():
                            sentences = abstract.split('.')
                            for sent in sentences:
                                if keyword in sent.lower():
                                    limitations.append(sent.strip())
                    result["extracted_info"]["limitations"] = limitations if limitations else ["éœ€è¦å®Œæ•´æ–‡æœ¬åˆ†æ"]
                
                else:
                    result["extracted_info"][info_type] = f"{info_type}ä¿¡æ¯æŠ½å–éœ€è¦å®Œæ•´PDFæ–‡æœ¬"
            
            return result
        else:
            return {"error": f"API error: {response.status_code}"}
            
    except Exception as e:
        return {"error": f"Error extracting information: {str(e)}"}


# ============================================================================
# 3. å®Œæ•´å·¥ä½œæµå·¥å…·
# ============================================================================

@mcp.tool()
async def research_workflow_complete(
    topic: str,
    num_papers: int = 5,
    include_analysis: bool = True,
    include_summary: bool = True
) -> Dict[str, Any]:
    """
    æ‰§è¡Œå®Œæ•´çš„ç ”ç©¶å·¥ä½œæµï¼šæ£€ç´¢ â†’ åˆ†æ â†’ é˜…è¯» â†’ æ€»ç»“
    
    è¿™æ˜¯æ¨èçš„æ ¸å¿ƒåŠŸèƒ½ï¼ä¸€é”®å®Œæˆæ•´ä¸ªç ”ç©¶æµç¨‹ã€‚
    
    Args:
        topic: ç ”ç©¶ä¸»é¢˜ï¼ˆä¾‹å¦‚ï¼š"CRISPR gene editing"ã€"æ·±åº¦å­¦ä¹ "ï¼‰
        num_papers: æ£€ç´¢è®ºæ–‡æ•°é‡
        include_analysis: æ˜¯å¦åŒ…å«æ·±åº¦åˆ†æ
        include_summary: æ˜¯å¦åŒ…å«è‡ªåŠ¨æ‘˜è¦
    
    Returns:
        å®Œæ•´çš„ç ”ç©¶æŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
        - search_results: æ£€ç´¢ç»“æœ
        - paper_analyses: æ¯ç¯‡è®ºæ–‡çš„åˆ†æ
        - summaries: è®ºæ–‡æ‘˜è¦
        - overall_insights: æ•´ä½“è§è§£
        - recommendations: è¿›ä¸€æ­¥é˜…è¯»æ¨è
    """
    logging.info(f"Starting complete research workflow for topic: {topic}")
    
    try:
        workflow_result = {
            "topic": topic,
            "timestamp": "2025-10-24",
            "search_results": [],
            "paper_analyses": [],
            "summaries": [],
            "overall_insights": {},
            "recommendations": []
        }
        
        # æ­¥éª¤1: æ£€ç´¢è®ºæ–‡
        logging.info("Step 1: Searching papers...")
        search_url = f"https://api.semanticscholar.org/graph/v1/paper/search"
        params = {
            "query": topic,
            "limit": num_papers,
            "fields": "paperId,title,year,citationCount,influentialCitationCount,abstract,authors"
        }
        
        search_response = await asyncio.to_thread(requests.get, search_url, params=params)
        
        if search_response.status_code == 200:
            search_data = search_response.json()
            papers = search_data.get("data", [])
            workflow_result["search_results"] = papers
            
            # æ­¥éª¤2 & 3: åˆ†æå’Œæ€»ç»“æ¯ç¯‡è®ºæ–‡
            for paper in papers[:num_papers]:
                paper_id = paper.get("paperId")
                
                if include_analysis:
                    logging.info(f"Analyzing paper: {paper_id}")
                    impact = await evaluate_paper_impact(paper_id)
                    workflow_result["paper_analyses"].append({
                        "paper_id": paper_id,
                        "title": paper.get("title"),
                        "impact_analysis": impact
                    })
                
                if include_summary:
                    logging.info(f"Generating summary for: {paper_id}")
                    summary = await generate_paper_summary(paper_id, summary_type="brief")
                    workflow_result["summaries"].append(summary)
            
            # æ­¥éª¤4: ç”Ÿæˆæ•´ä½“è§è§£
            total_citations = sum(p.get("citationCount", 0) for p in papers)
            avg_citations = total_citations / len(papers) if papers else 0
            years = [p.get("year", 0) for p in papers if p.get("year")]
            
            workflow_result["overall_insights"] = {
                "total_papers_analyzed": len(papers),
                "total_citations": total_citations,
                "average_citations": round(avg_citations, 2),
                "year_range": f"{min(years)} - {max(years)}" if years else "N/A",
                "research_trend": "æ´»è·ƒ" if avg_citations > 50 else "æ–°å…´",
                "key_observations": [
                    f"åœ¨'{topic}'é¢†åŸŸæ‰¾åˆ° {len(papers)} ç¯‡ç›¸å…³è®ºæ–‡",
                    f"å¹³å‡è¢«å¼• {round(avg_citations, 1)} æ¬¡",
                    f"å‘è¡¨å¹´ä»½èŒƒå›´ï¼š{min(years) if years else 'N/A'} - {max(years) if years else 'N/A'}",
                    "å»ºè®®ä¼˜å…ˆé˜…è¯»é«˜å½±å“åŠ›è®ºæ–‡"
                ]
            }
            
            # æ­¥éª¤5: æ¨èè¿›ä¸€æ­¥é˜…è¯»
            if papers:
                top_paper_id = papers[0].get("paperId")
                workflow_result["recommendations"] = await recommend_related_papers(
                    top_paper_id,
                    num_recommendations=5,
                    strategy="influential"
                )
            
            return workflow_result
        else:
            return {"error": f"Search failed: {response.status_code}"}
            
    except Exception as e:
        return {"error": f"Workflow error: {str(e)}"}


@mcp.tool()
async def compare_papers(
    paper_ids: List[str],
    comparison_aspects: List[str] = None
) -> Dict[str, Any]:
    """
    å¯¹æ¯”å¤šç¯‡è®ºæ–‡
    
    Args:
        paper_ids: è¦å¯¹æ¯”çš„è®ºæ–‡IDåˆ—è¡¨ï¼ˆ2-5ç¯‡ï¼‰
        comparison_aspects: å¯¹æ¯”ç»´åº¦
            - "methodology": ç ”ç©¶æ–¹æ³•å¯¹æ¯”
            - "findings": ç ”ç©¶å‘ç°å¯¹æ¯”
            - "impact": å½±å“åŠ›å¯¹æ¯”
            - "timeline": æ—¶é—´çº¿å¯¹æ¯”
    
    Returns:
        è¯¦ç»†çš„å¯¹æ¯”åˆ†ææŠ¥å‘Š
    """
    if comparison_aspects is None:
        comparison_aspects = ["methodology", "findings", "impact"]
    
    logging.info(f"Comparing {len(paper_ids)} papers")
    
    try:
        comparison_result = {
            "papers_compared": len(paper_ids),
            "comparison_aspects": comparison_aspects,
            "papers": [],
            "comparative_analysis": {}
        }
        
        # è·å–æ‰€æœ‰è®ºæ–‡çš„ä¿¡æ¯
        for paper_id in paper_ids:
            base_url = "https://api.semanticscholar.org/graph/v1"
            fields = "title,year,abstract,citationCount,influentialCitationCount,authors,fieldsOfStudy"
            url = f"{base_url}/paper/{paper_id}?fields={fields}"
            
            response = await asyncio.to_thread(requests.get, url)
            if response.status_code == 200:
                paper_data = response.json()
                comparison_result["papers"].append(paper_data)
        
        # è¿›è¡Œå¯¹æ¯”åˆ†æ
        if "impact" in comparison_aspects:
            citations = [p.get("citationCount", 0) for p in comparison_result["papers"]]
            comparison_result["comparative_analysis"]["impact"] = {
                "highest_cited": max(citations) if citations else 0,
                "lowest_cited": min(citations) if citations else 0,
                "citation_range": max(citations) - min(citations) if citations else 0,
                "most_influential_paper": comparison_result["papers"][citations.index(max(citations))].get("title") if citations else None
            }
        
        if "timeline" in comparison_aspects:
            years = [p.get("year", 0) for p in comparison_result["papers"] if p.get("year")]
            comparison_result["comparative_analysis"]["timeline"] = {
                "earliest_year": min(years) if years else None,
                "latest_year": max(years) if years else None,
                "time_span": max(years) - min(years) if years and len(years) > 1 else 0
            }
        
        # é¢†åŸŸå¯¹æ¯”
        all_fields = set()
        for paper in comparison_result["papers"]:
            all_fields.update(paper.get("fieldsOfStudy", []))
        
        comparison_result["comparative_analysis"]["fields"] = {
            "common_fields": list(all_fields),
            "interdisciplinary": len(all_fields) > 3
        }
        
        return comparison_result
        
    except Exception as e:
        return {"error": f"Comparison error: {str(e)}"}


# ============================================================================
# 4. æœ¬åœ°PDFåˆ†æå·¥å…·ï¼ˆæ”¯æŒæœ¬åœ°æ–‡ä»¶+åœ¨çº¿URLï¼‰
# ============================================================================

@mcp.tool()
async def list_all_figures(
    pdf_path: str
) -> Dict[str, Any]:
    """
    åˆ—å‡ºè®ºæ–‡ä¸­æ‰€æœ‰å›¾è¡¨çš„æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯ï¼ˆæ”¯æŒæœ¬åœ°PDFå’Œåœ¨çº¿URLï¼‰
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
            - æœ¬åœ°: "C:\\Papers\\paper.pdf" æˆ– "/home/user/paper.pdf"
            - URL: "https://arxiv.org/pdf/1706.03762.pdf"
    
    Returns:
        æ‰€æœ‰å›¾è¡¨çš„åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«ç¼–å·å’Œæ ‡é¢˜
    """
    logging.info(f"Listing all figures from {pdf_path}")
    
    try:
        import PyPDF2
        
        # è¯»å–PDFï¼ˆè‡ªåŠ¨åˆ¤æ–­æœ¬åœ°è¿˜æ˜¯URLï¼‰
        pdf_content = await _read_pdf_content(pdf_path)
        pdf_reader = PyPDF2.PdfReader(pdf_content)
        
        # æå–æ‰€æœ‰æ–‡æœ¬
        full_text = ""
        for page in pdf_reader.pages:
            full_text += page.extract_text()
        
        # æŸ¥æ‰¾æ‰€æœ‰å›¾è¡¨æ ‡é¢˜
        figure_pattern = r'(Figure|Fig\.|å›¾|Table|è¡¨)\s*(\d+)[:\.]?\s*([^\n]+)'
        matches = re.findall(figure_pattern, full_text, re.IGNORECASE)
        
        figures_list = []
        seen = set()
        
        for match in matches:
            fig_type, fig_num, caption = match
            fig_id = f"{fig_type.lower()}-{fig_num}"
            
            if fig_id not in seen:
                seen.add(fig_id)
                figures_list.append({
                    "type": fig_type,
                    "number": int(fig_num),
                    "caption": caption.strip()[:200],  # é™åˆ¶é•¿åº¦
                    "full_id": f"{fig_type} {fig_num}"
                })
        
        # æŒ‰ç¼–å·æ’åº
        figures_list.sort(key=lambda x: (x["type"], x["number"]))
        
        return {
            "source_type": "local" if _is_local_path(pdf_path) else "url",
            "source_path": pdf_path,
            "total_figures": len(figures_list),
            "figures": figures_list,
            "tip": "ä½¿ç”¨ explain_specific_figure è·å–å•ä¸ªå›¾è¡¨çš„è¯¦ç»†è§£é‡Š"
        }
        
    except FileNotFoundError:
        return {
            "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}",
            "suggestion": "è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®"
        }
    except ImportError:
        return {
            "error": "éœ€è¦å®‰è£… PyPDF2",
            "install_command": "pip install PyPDF2"
        }
    except Exception as e:
        return {"error": f"Error listing figures: {str(e)}"}


@mcp.tool()
async def explain_specific_figure(
    pdf_path: str,
    figure_number: int,
    provide_context: bool = True
) -> Dict[str, Any]:
    """
    è§£é‡Šè®ºæ–‡ä¸­çš„ç‰¹å®šå›¾è¡¨ï¼ˆæ”¯æŒæœ¬åœ°PDFå’Œåœ¨çº¿URLï¼Œä»…è¿”å›æ–‡å­—è§£é‡Šï¼‰
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„ï¼ˆæœ¬åœ°æˆ–URLï¼‰
        figure_number: å›¾è¡¨ç¼–å·ï¼ˆå¦‚ 1, 2, 3ï¼‰
        provide_context: æ˜¯å¦æä¾›ç›¸å…³æ®µè½çš„ä¸Šä¸‹æ–‡
    
    Examples:
        æœ¬åœ°æ–‡ä»¶:
        explain_specific_figure(
            pdf_path="C:\\Papers\\my_paper.pdf",
            figure_number=1
        )
        
        URL:
        explain_specific_figure(
            pdf_path="https://arxiv.org/pdf/1706.03762.pdf",
            figure_number=1
        )
    
    Returns:
        å•ä¸ªå›¾è¡¨çš„è¯¦ç»†æ–‡å­—è§£é‡Š
    """
    logging.info(f"Explaining Figure {figure_number} from {pdf_path}")
    
    try:
        import PyPDF2
        
        # è¯»å–PDF
        pdf_content = await _read_pdf_content(pdf_path)
        pdf_reader = PyPDF2.PdfReader(pdf_content)
        
        # æå–æ–‡æœ¬
        full_text = ""
        for page in pdf_reader.pages:
            full_text += page.extract_text()
        
        # æŸ¥æ‰¾ç‰¹å®šå›¾è¡¨
        figure_pattern = rf'(Figure|Fig\.|å›¾)\s*{figure_number}[:\.]?\s*([^\n]+)'
        match = re.search(figure_pattern, full_text, re.IGNORECASE)
        
        if not match:
            return {
                "error": f"æœªæ‰¾åˆ°Figure {figure_number}",
                "suggestion": "è¯·æ£€æŸ¥å›¾è¡¨ç¼–å·æ˜¯å¦æ­£ç¡®ï¼Œæˆ–PDFæ˜¯å¦åŒ…å«å¯æå–çš„æ–‡æœ¬"
            }
        
        caption = match.group(2).strip()
        
        # åˆ¤æ–­å›¾è¡¨ç±»å‹
        figure_type = "unknown"
        if any(word in caption.lower() for word in ["graph", "plot", "curve", "æ›²çº¿"]):
            figure_type = "graph/plot"
        elif any(word in caption.lower() for word in ["table", "è¡¨æ ¼"]):
            figure_type = "table"
        elif any(word in caption.lower() for word in ["diagram", "schematic", "ç¤ºæ„å›¾", "æµç¨‹"]):
            figure_type = "diagram/schematic"
        elif any(word in caption.lower() for word in ["image", "photo", "microscopy", "æ˜¾å¾®", "ç…§ç‰‡"]):
            figure_type = "image/photo"
        elif any(word in caption.lower() for word in ["chart", "bar", "æŸ±çŠ¶", "é¥¼å›¾"]):
            figure_type = "chart"
        
        # ç”Ÿæˆè¯¦ç»†è§£é‡Š
        explanation = f"""
å›¾{figure_number} è§£é‡Šï¼š

ğŸ“Š å›¾è¡¨ç±»å‹: {figure_type}

ğŸ“ æ ‡é¢˜å†…å®¹: {caption}

ğŸ” è¯¦ç»†åˆ†æ:
"""
        
        # åŸºäºæ ‡é¢˜å…³é”®è¯ç”Ÿæˆé’ˆå¯¹æ€§è§£é‡Š
        if "comparison" in caption.lower() or "compare" in caption.lower() or "å¯¹æ¯”" in caption:
            explanation += """
è¿™æ˜¯ä¸€ä¸ªå¯¹æ¯”ç±»å›¾è¡¨ï¼Œç”¨äºæ¯”è¾ƒä¸åŒæ¡ä»¶ã€æ–¹æ³•æˆ–ç»„åˆ«ä¹‹é—´çš„å·®å¼‚ã€‚
- å…³æ³¨å„ç»„ä¹‹é—´çš„æ•°å€¼å·®å¼‚
- æ³¨æ„æ˜¾è‘—æ€§æ ‡è®°ï¼ˆå¦‚ *, **, ***ï¼‰
- æ¯”è¾ƒè¶‹åŠ¿å’Œæ¨¡å¼çš„å¼‚åŒ
"""
        
        elif "workflow" in caption.lower() or "pipeline" in caption.lower() or "æµç¨‹" in caption:
            explanation += """
è¿™æ˜¯ä¸€ä¸ªå·¥ä½œæµç¨‹å›¾ï¼Œå±•ç¤ºäº†ç ”ç©¶æ–¹æ³•æˆ–å®éªŒæ­¥éª¤ã€‚
- æŒ‰ç…§ç®­å¤´æ–¹å‘ç†è§£å¤„ç†æµç¨‹
- æ³¨æ„æ¯ä¸ªæ­¥éª¤çš„è¾“å…¥å’Œè¾“å‡º
- ç†è§£å„æ­¥éª¤ä¹‹é—´çš„é€»è¾‘å…³ç³»
"""
        
        elif "result" in caption.lower() or "data" in caption.lower() or "ç»“æœ" in caption:
            explanation += """
è¿™æ˜¯ä¸€ä¸ªç»“æœå±•ç¤ºå›¾ï¼Œå‘ˆç°äº†å®éªŒæˆ–åˆ†æçš„ä¸»è¦å‘ç°ã€‚
- å…³æ³¨æ•°æ®çš„è¶‹åŠ¿å’Œæ¨¡å¼
- æ³¨æ„è¯¯å·®èŒƒå›´æˆ–ç½®ä¿¡åŒºé—´
- æ¯”å¯¹å®éªŒç»„å’Œå¯¹ç…§ç»„çš„å·®å¼‚
"""
        
        elif "structure" in caption.lower() or "model" in caption.lower() or "ç»“æ„" in caption:
            explanation += """
è¿™æ˜¯ä¸€ä¸ªç»“æ„ç¤ºæ„å›¾ï¼Œå±•ç¤ºäº†ç³»ç»Ÿæˆ–æ¨¡å‹çš„ç»„æˆã€‚
- ç†è§£å„ä¸ªç»„ä»¶çš„åŠŸèƒ½å’Œä½ç½®
- æ³¨æ„ç»„ä»¶ä¹‹é—´çš„è¿æ¥å…³ç³»
- æŠŠæ¡æ•´ä½“æ¶æ„çš„è®¾è®¡æ€è·¯
"""
        
        else:
            explanation += """
æ ¹æ®æ ‡é¢˜åˆ†æï¼Œè¿™ä¸ªå›¾è¡¨å±•ç¤ºäº†ç ”ç©¶çš„å…³é”®ä¿¡æ¯ã€‚
å»ºè®®ç»“åˆè®ºæ–‡æ­£æ–‡ç›¸å…³æ®µè½ç†è§£å›¾è¡¨çš„å…·ä½“å«ä¹‰ã€‚
"""
        
        result = {
            "source_type": "local" if _is_local_path(pdf_path) else "url",
            "figure_number": figure_number,
            "caption": caption,
            "figure_type": figure_type,
            "explanation": explanation.strip()
        }
        
        if provide_context:
            result["usage_tips"] = [
                "ç»“åˆè®ºæ–‡Methodséƒ¨åˆ†ç†è§£å®éªŒè®¾è®¡",
                "å¯¹ç…§Resultséƒ¨åˆ†çš„æ–‡å­—æè¿°",
                "æ³¨æ„ä¸å…¶ä»–å›¾è¡¨çš„å…³è”æ€§",
                "å…³æ³¨ç»Ÿè®¡æ˜¾è‘—æ€§æ ‡è®°"
            ]
        
        return result
        
    except ImportError:
        return {
            "error": "éœ€è¦å®‰è£… PyPDF2",
            "install_command": "pip install PyPDF2"
        }
    except Exception as e:
        return {"error": f"Error: {str(e)}"}


@mcp.tool()
async def analyze_local_paper(
    pdf_path: str,
    include_figures: bool = True,
    include_summary: bool = True
) -> Dict[str, Any]:
    """
    å®Œæ•´åˆ†ææœ¬åœ°æˆ–åœ¨çº¿PDFè®ºæ–‡
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„ï¼ˆæœ¬åœ°æˆ–URLï¼‰
        include_figures: æ˜¯å¦åˆ†æå›¾è¡¨
        include_summary: æ˜¯å¦ç”Ÿæˆæ‘˜è¦
    
    Returns:
        å®Œæ•´çš„è®ºæ–‡åˆ†ææŠ¥å‘Š
    """
    logging.info(f"Analyzing paper: {pdf_path}")
    
    try:
        import PyPDF2
        
        # è¯»å–PDF
        pdf_content = await _read_pdf_content(pdf_path)
        pdf_reader = PyPDF2.PdfReader(pdf_content)
        
        result = {
            "source_type": "local" if _is_local_path(pdf_path) else "url",
            "file_path": pdf_path,
            "file_name": os.path.basename(pdf_path) if _is_local_path(pdf_path) else pdf_path.split('/')[-1],
            "pages": len(pdf_reader.pages),
            "analysis": {}
        }
        
        # æå–æ–‡æœ¬
        full_text = ""
        for page in pdf_reader.pages:
            full_text += page.extract_text()
        
        result["word_count"] = len(full_text.split())
        
        # æå–æ ‡é¢˜ï¼ˆé€šå¸¸åœ¨ç¬¬ä¸€é¡µï¼‰
        first_page_text = pdf_reader.pages[0].extract_text()
        title_lines = first_page_text.split('\n')[:5]
        result["potential_title"] = " ".join(title_lines)
        
        # åˆ†æå›¾è¡¨
        if include_figures:
            figures_data = await list_all_figures(pdf_path)
            result["analysis"]["figures"] = figures_data
        
        # ç”Ÿæˆæ‘˜è¦
        if include_summary:
            # æå–æ‘˜è¦éƒ¨åˆ†
            abstract_pattern = r'Abstract[:\.]?\s*(.*?)(?=\n\n|\nIntroduction|\n1\.)'
            abstract_match = re.search(abstract_pattern, full_text, re.IGNORECASE | re.DOTALL)
            
            if abstract_match:
                result["analysis"]["abstract"] = abstract_match.group(1).strip()[:500]
            else:
                result["analysis"]["abstract"] = "æœªæ‰¾åˆ°æ‘˜è¦éƒ¨åˆ†"
        
        # æå–å…³é”®éƒ¨åˆ†
        sections = ["Introduction", "Method", "Result", "Discussion", "Conclusion"]
        found_sections = []
        for section in sections:
            if section.lower() in full_text.lower():
                found_sections.append(section)
        
        result["analysis"]["sections_found"] = found_sections
        
        return result
        
    except FileNotFoundError:
        return {"error": f"æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}"}
    except ImportError:
        return {
            "error": "éœ€è¦å®‰è£… PyPDF2",
            "install_command": "pip install PyPDF2"
        }
    except Exception as e:
        return {"error": f"Analysis error: {str(e)}"}


@mcp.tool()
async def batch_analyze_local_papers(
    folder_path: str,
    max_papers: int = 10,
    file_pattern: str = "*.pdf"
) -> Dict[str, Any]:
    """
    æ‰¹é‡åˆ†ææ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰PDFè®ºæ–‡ï¼ˆä»…æ”¯æŒæœ¬åœ°æ–‡ä»¶å¤¹ï¼‰
    
    Args:
        folder_path: æ–‡ä»¶å¤¹è·¯å¾„
        max_papers: æœ€å¤šåˆ†æçš„è®ºæ–‡æ•°é‡
        file_pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼ï¼ˆé»˜è®¤ "*.pdf"ï¼‰
    
    Returns:
        æ‰€æœ‰è®ºæ–‡çš„åˆ†æç»“æœ
    """
    logging.info(f"Batch analyzing papers in: {folder_path}")
    
    if not os.path.exists(folder_path):
        return {"error": f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}"}
    
    try:
        # æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
        pdf_files = list(Path(folder_path).glob(file_pattern))[:max_papers]
        
        if not pdf_files:
            return {
                "error": "æœªæ‰¾åˆ°PDFæ–‡ä»¶",
                "folder": folder_path,
                "pattern": file_pattern
            }
        
        result = {
            "folder_path": folder_path,
            "total_files": len(pdf_files),
            "papers": []
        }
        
        # åˆ†ææ¯ç¯‡è®ºæ–‡
        for pdf_path in pdf_files:
            logging.info(f"Analyzing: {pdf_path.name}")
            
            paper_analysis = await analyze_local_paper(
                str(pdf_path),
                include_figures=True,
                include_summary=True
            )
            
            result["papers"].append(paper_analysis)
        
        return result
        
    except Exception as e:
        return {"error": f"Batch analysis error: {str(e)}"}


@mcp.tool()
async def extract_text_from_pdf(
    pdf_path: str,
    extract_sections: bool = True,
    page_range: Optional[tuple] = None
) -> Dict[str, Any]:
    """
    ä»PDFæå–æ–‡æœ¬å†…å®¹ï¼ˆæ”¯æŒæœ¬åœ°å’Œåœ¨çº¿URLï¼‰
    
    Args:
        pdf_path: PDFè·¯å¾„ï¼ˆæœ¬åœ°æˆ–URLï¼‰
        extract_sections: æ˜¯å¦æŒ‰ç« èŠ‚æå–
        page_range: é¡µé¢èŒƒå›´ï¼Œå¦‚ (1, 10) è¡¨ç¤ºç¬¬1-10é¡µ
    
    Returns:
        æå–çš„æ–‡æœ¬å†…å®¹
    """
    logging.info(f"Extracting text from: {pdf_path}")
    
    try:
        import PyPDF2
        
        # è¯»å–PDF
        pdf_content = await _read_pdf_content(pdf_path)
        pdf_reader = PyPDF2.PdfReader(pdf_content)
        total_pages = len(pdf_reader.pages)
        
        # ç¡®å®šé¡µé¢èŒƒå›´
        if page_range:
            start, end = page_range
            start = max(0, start - 1)  # è½¬æ¢ä¸º0-basedç´¢å¼•
            end = min(total_pages, end)
        else:
            start, end = 0, total_pages
        
        # æå–æ–‡æœ¬
        full_text = ""
        for i in range(start, end):
            full_text += pdf_reader.pages[i].extract_text()
        
        result = {
            "source_type": "local" if _is_local_path(pdf_path) else "url",
            "file_path": pdf_path,
            "total_pages": total_pages,
            "extracted_pages": f"{start+1}-{end}",
            "word_count": len(full_text.split()),
            "full_text": full_text[:5000]  # é™åˆ¶è¿”å›é•¿åº¦ï¼Œé¿å…è¿‡å¤§
        }
        
        # æå–ç« èŠ‚
        if extract_sections:
            sections = {}
            section_patterns = {
                "abstract": r'Abstract[:\.]?\s*(.*?)(?=\n\n|\nIntroduction)',
                "introduction": r'Introduction[:\.]?\s*(.*?)(?=\n\n|\nMethod)',
                "methods": r'Method[s]?[:\.]?\s*(.*?)(?=\n\n|\nResult)',
                "results": r'Result[s]?[:\.]?\s*(.*?)(?=\n\n|\nDiscussion)',
                "discussion": r'Discussion[:\.]?\s*(.*?)(?=\n\n|\nConclusion)',
                "conclusion": r'Conclusion[s]?[:\.]?\s*(.*?)(?=\n\n|\nReference)'
            }
            
            for section_name, pattern in section_patterns.items():
                match = re.search(pattern, full_text, re.IGNORECASE | re.DOTALL)
                if match:
                    sections[section_name] = match.group(1).strip()[:1000]  # é™åˆ¶é•¿åº¦
            
            result["sections"] = sections
        
        return result
        
    except FileNotFoundError:
        return {"error": f"æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}"}
    except ImportError:
        return {
            "error": "éœ€è¦å®‰è£… PyPDF2",
            "install_command": "pip install PyPDF2"
        }
    except Exception as e:
        return {"error": f"Text extraction error: {str(e)}"}


if __name__ == "__main__":
    logging.info("Starting Academic Research Advanced MCP Server")
    logging.info("Supports both local PDF files and online URLs")
    logging.info("Available tools: citation network, impact evaluation, recommendations, summaries, workflows, local PDF analysis")
    mcp.run(transport='stdio')

